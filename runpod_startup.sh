#!/bin/bash
set -e

echo "üöÄ Starting Qwen-Image Setup on RunPod"
echo "=========================================="

# Update system
echo "üì¶ Updating system packages..."
apt-get update -qq
apt-get install -y git wget curl

# Clone official Qwen-Image repository
echo "üì• Cloning official Qwen-Image repository..."
cd /workspace
if [ ! -d "Qwen-Image" ]; then
    git clone https://github.com/QwenLM/Qwen-Image.git
fi
cd Qwen-Image

# Install Python dependencies with PROPER VERSIONS
echo "üêç Installing Python dependencies with verified compatibility..."

# Upgrade pip first
pip install --upgrade pip

echo "üî• Installing PyTorch STABLE 2.5.1 (cu124) for DFloat11..."
# Force replace nightly/dev builds that break DFloat11 kernels
pip uninstall -y torch torchvision torchaudio || true
pip install --index-url https://download.pytorch.org/whl/cu124 \
  --no-cache-dir --force-reinstall --upgrade \
  torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1

# Clean up immediately after install to save space
pip cache purge
echo "üìä Disk usage after PyTorch install:"
echo "Container:" && df -h / || true
echo "Volume:" && df -h /workspace || true

echo "ü§ó Installing Hugging Face libraries with correct versions..."
# Qwen-Image specifically requires transformers>=4.51.3 for Qwen2.5-VL support  
pip install "transformers>=4.51.3" --no-cache-dir
pip install "accelerate>=0.26.1" --no-cache-dir
pip install "safetensors>=0.3.1" --no-cache-dir

# Clean cache after each install
pip cache purge

echo "üî• Installing DFloat11 for lossless compression (32% smaller, 100% quality)..."
pip install -U --force-reinstall "dfloat11[cuda12]" --no-cache-dir
pip cache purge

echo "üé® Installing latest Diffusers from source..."
# Always use latest diffusers for best Qwen-Image support
pip install git+https://github.com/huggingface/diffusers --no-cache-dir
pip cache purge

echo "üöÄ Installing FastAPI stack..."
pip install "fastapi>=0.100.0" "uvicorn[standard]>=0.23.0" "pydantic>=2.0.0" --no-cache-dir

echo "üñºÔ∏è Installing image processing libraries..."
pip install "pillow>=10.0.0" requests --no-cache-dir

# Final cache cleanup
pip cache purge
echo "üìä Final disk usage after installs:"
echo "Container:" && df -h / || true
echo "Volume (critical):" && df -h /workspace || true

echo "üåç Setting up environment variables..."
export HF_HOME=/workspace/.cache/huggingface  # Replace deprecated TRANSFORMERS_CACHE
export TRANSFORMERS_CACHE=/workspace/.cache/huggingface  # Fallback for compatibility
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:512"  # Fix memory fragmentation
export HF_HOME=/workspace/.cache/huggingface
export TRANSFORMERS_CACHE=/workspace/.cache/huggingface
export CUDA_LAUNCH_BLOCKING=0  # For better performance

echo "üßπ EMERGENCY: Cleaning disk space and GPU processes..."
# Kill any existing Python processes
pkill -f "python.*qwen" || true
pkill -f "uvicorn.*qwen" || true

# AGGRESSIVE DISK CLEANUP for RunPod VOLUME (not container!)
echo "üíæ Freeing up VOLUME disk space at /workspace..."

# Clean the VOLUME disk where work actually happens
rm -rf /workspace/.cache/* || true
rm -rf /workspace/Qwen-Image/.git || true  # Remove git history (saves ~100MB)
rm -rf /workspace/*.log || true
rm -rf /workspace/temp* || true
rm -rf /workspace/tmp* || true

# Clean container disk too (but this isn't the main issue)
apt-get clean || true
rm -rf /var/lib/apt/lists/* || true
rm -rf /tmp/* || true
rm -rf /var/tmp/* || true

# Clean Python cache (both locations)
pip cache purge || true
python -m pip cache purge || true
rm -rf ~/.cache/pip || true

# Clean conda if exists
conda clean -all -y || true

# Clear GPU memory
nvidia-smi --gpu-reset-gpus=$(nvidia-smi --query-gpu=index --format=csv,noheader,nounits) || true

echo "üìä Disk usage after cleanup:"
echo "Container disk:"
df -h /
echo "Volume disk (where work happens):"
df -h /workspace || echo "Volume not mounted yet"
sleep 2

echo "üö´ SKIPPING FlashAttention-2 - causes crashes with PyTorch 2.8 dev even when gracefully handled"
echo "‚ö° Using native PyTorch attention (100% reliable, still fast!)"

# Create our simple API wrapper
echo "üîß Creating FastAPI wrapper..."
echo "üßπ FINAL cleanup before writing API file..."
# Emergency volume cleanup right before file creation
rm -rf /workspace/.cache/huggingface/hub/models--* || true
df -h /workspace || true
cat > /workspace/qwen_api.py << 'EOF'
#!/usr/bin/env python3
"""
Simple FastAPI wrapper for Qwen-Image
No authentication required - clean and simple!
"""
import os
import sys
import logging
import asyncio
from typing import Optional, List
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import torch
import uvicorn
from PIL import Image
import base64
import io

# Force disable FlashAttention to prevent diffusers from auto-importing it
import os
os.environ["FLASH_ATTENTION_FORCE_DISABLE"] = "1"
os.environ["DISABLE_FLASH_ATTN"] = "1"

# FlashAttention causes PyTorch 2.8 dev crashes - using native attention
FLASH_ATTN_AVAILABLE = False
print("‚ö° Using native PyTorch attention (100% compatible, still excellent!)")
print("üöÄ OPTIMIZED FOR L40S: Running DFloat11 Qwen-Image - lossless compression + 100% quality!")
print("üí™ DFloat11: 28.42GB model size, ~30GB peak VRAM vs L40S 48GB = PERFECT FIT!")

from diffusers import DiffusionPipeline

# CRITICAL: Clear any existing GPU memory first
import torch
import gc
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    gc.collect()
    print(f"üßπ Cleared GPU memory. Available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# CRITICAL: Set memory management environment variables BEFORE any model loading
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:512"
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"  # For better performance

# Global pipeline
pipeline = None

# Modern FastAPI lifespan handler (replaces deprecated on_event)
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global pipeline
    
    logger.info("üöÄ Loading DFloat11 Qwen-Image - LOSSLESS 32% compression + 100% quality for L40S!")
    
    # CRITICAL: Maximum memory cleanup before loading
    import gc
    torch.cuda.empty_cache()
    gc.collect()
    logger.info(f"üßπ Pre-load cleanup: {torch.cuda.memory_allocated() / 1024**3:.1f}GB used, {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3:.1f}GB free")
    
    # Load the pipeline with optimal settings for Qwen-Image
    try:
        
        # Load DFloat11 compressed Qwen-Image - LOSSLESS 32% smaller + 100% quality!
        from transformers.modeling_utils import no_init_weights
        from dfloat11 import DFloat11Model
        from diffusers import QwenImageTransformer2DModel
        
        # EXACT OFFICIAL PATTERN: 32GB+ VRAM case (L40S has 48GB)
        model_name = "Qwen/Qwen-Image"
        
        # Step 1: Load transformer config exactly as docs show
        with no_init_weights():
            transformer = QwenImageTransformer2DModel.from_config(
                QwenImageTransformer2DModel.load_config(
                    model_name, subfolder="transformer",
                ),
            ).to(torch.bfloat16)
        
        # Step 2: EXACT official DFloat11 loading - CRITICAL FIX
        # The DFloat11Model.from_pretrained modifies transformer IN-PLACE
        logger.info("üì¶ Loading DFloat11 compressed weights...")
        compressed_model = DFloat11Model.from_pretrained(
            "DFloat11/Qwen-Image-DF11",
            device="cpu",  # Official: always CPU first
            cpu_offload=False,  # 32GB+ case 
            pin_memory=True,  # kernel path validated on 2.5.1
            bfloat16_model=transformer,
        )
        logger.info(f"üìä DFloat11 model loaded, checking compression...")
        
        # Step 3: Create pipeline with DFloat11-modified transformer
        logger.info("üîß Creating pipeline with compressed transformer...")
        pipeline = DiffusionPipeline.from_pretrained(
            model_name,
            transformer=transformer,  # This should now contain DFloat11 compressed weights
            torch_dtype=torch.bfloat16,
        )
        
        # CRITICAL: Verify the transformer actually has compressed weights
        transformer_memory = sum(p.numel() * p.element_size() for p in transformer.parameters()) / (1024**3)
        logger.info(f"üßÆ Transformer memory usage: {transformer_memory:.2f} GB")
        if transformer_memory > 35:  # If > 35GB, compression failed
            logger.error("‚ùå DFloat11 compression FAILED - transformer still full size!")
            raise RuntimeError("DFloat11 compression did not work")
        
        # Step 4: EXACT official pattern - ALWAYS use enable_model_cpu_offload()
        # This is required for DFloat11 to work properly (even for 48GB VRAM)
        pipeline.enable_model_cpu_offload()
        logger.info("‚úÖ DFloat11 loaded using EXACT official pattern")
        logger.info("‚úÖ Model size should be 28.42GB (not 40GB)")
        
        # VERIFY: Check all components are on CUDA
        if hasattr(pipeline, 'transformer') and pipeline.transformer is not None:
            transformer_device = next(pipeline.transformer.parameters()).device
            logger.info(f"üîç Transformer device: {transformer_device}")
        
        if hasattr(pipeline, 'vae') and pipeline.vae is not None:
            vae_device = next(pipeline.vae.parameters()).device
            logger.info(f"üîç VAE device: {vae_device}")
            
        logger.info("‚úÖ Qwen-Image model loaded successfully!")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load DFloat11 Qwen-Image: {e}")
        logger.info("üîß Trying progressive fallback methods...")
        
        # Fallback 1: DFloat11 with memory cleanup
        try:
            logger.info("üìã Fallback 1: DFloat11 with memory cleanup...")
            # Clear GPU memory first
            torch.cuda.empty_cache()
            gc.collect()
            
            # Fallback: Try official pattern with cleanup
            model_name = "Qwen/Qwen-Image"
            
            with no_init_weights():
                transformer = QwenImageTransformer2DModel.from_config(
                    QwenImageTransformer2DModel.load_config(
                        model_name, subfolder="transformer",
                    ),
                ).to(torch.bfloat16)
            
            # Official DFloat11 loading (32GB+ case)
            DFloat11Model.from_pretrained(
                "DFloat11/Qwen-Image-DF11",
                device="cpu",  # Official: always "cpu"
                cpu_offload=False,  # 32GB+ case
                cpu_offload_blocks=None,
                pin_memory=False,  # Conservative for fallback
                bfloat16_model=transformer,
            )
            
            pipeline = DiffusionPipeline.from_pretrained(
                model_name,
                transformer=transformer,
                torch_dtype=torch.bfloat16,
            )
            
            # EXACT official pattern - use enable_model_cpu_offload()
            pipeline.enable_model_cpu_offload()
            
            # VERIFY: Check device placement
            if hasattr(pipeline, 'transformer') and pipeline.transformer is not None:
                transformer_device = next(pipeline.transformer.parameters()).device
                logger.info(f"üîç Fallback transformer device: {transformer_device}")
                
            logger.info("‚úÖ DFloat11 loaded with memory optimization")
            
        except Exception as e1:
            logger.error(f"‚ùå DFloat11 fallback failed: {e1}")
            
            # Fallback 2: Distilled diffusers model (smaller, GPU only)
            try:
                logger.info("üìã Fallback 2: Qwen-Image Distill (GPU only)...")
                torch.cuda.empty_cache()
                gc.collect()
                
                pipeline = DiffusionPipeline.from_pretrained(
                    "DiffSynth-Studio/Qwen-Image-Distill-Full",  # Distilled, smaller model
                    torch_dtype=torch.bfloat16,
                    low_cpu_mem_usage=True,  # safe
                    device_map="cuda",      # avoid 'auto not supported' on this stack
                    use_safetensors=True
                )
                
                # Keep on GPU for speed (distill fits easily)
                if hasattr(pipeline, "to"):
                    pipeline = pipeline.to("cuda")
                
                # VERIFY: Check device placement
                if hasattr(pipeline, 'transformer') and pipeline.transformer is not None:
                    transformer_device = next(pipeline.transformer.parameters()).device
                    logger.info(f"üîç Original model transformer device: {transformer_device}")
                    
                logger.info("‚úÖ Qwen-Image Distill loaded on GPU")
                
            except Exception as e2:
                logger.error(f"‚ùå GPU-only loading failed: {e2}")
                logger.error("üí• L40S memory exhausted. Try restarting container or reducing other processes")
                raise e2
    
    # Model device placement is handled by device_map="auto"
    if torch.cuda.is_available():
        logger.info(f"‚úÖ Model loaded with auto device mapping")
        logger.info(f"üî• GPU: {torch.cuda.get_device_name()}")
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_allocated = torch.cuda.memory_allocated() / 1024**3
        gpu_free = gpu_memory - gpu_allocated
        logger.info(f"üìä GPU Memory: {gpu_memory:.1f}GB total, {gpu_allocated:.1f}GB used, {gpu_free:.1f}GB free")
    else:
        logger.info("‚ö†Ô∏è Running on CPU (will be slow)")
        
    # Enable ALL memory optimizations for better VRAM usage
    if hasattr(pipeline, 'enable_attention_slicing'):
        pipeline.enable_attention_slicing()
        logger.info("‚úÖ Attention slicing enabled")
        
    if hasattr(pipeline, 'enable_vae_slicing'):
        pipeline.enable_vae_slicing()
        logger.info("‚úÖ VAE slicing enabled")
        
    if hasattr(pipeline, 'enable_vae_tiling'):
        pipeline.enable_vae_tiling()
        logger.info("‚úÖ VAE tiling enabled")
            
    # L40S CUDA-ONLY: Maximum performance with 48GB VRAM
    logger.info("üöÄ L40S CUDA-ONLY: Entire model on GPU for maximum performance!")
    logger.info("üí™ 48GB VRAM + DFloat11 compression = Perfect fit!")
    logger.info("‚ö° No CPU offload - pure GPU power!")
    
    # Always using native attention for PyTorch 2.8 stability
    logger.info("‚ö° Native PyTorch attention active - 100% stable and fast!")
        
    # Aggressive memory cleanup after model loading
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gpu_free_after = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3
        logger.info(f"‚úÖ GPU memory cleaned. Free memory: {gpu_free_after:.1f}GB")
        
    logger.info("üéâ Qwen-Image model loaded successfully!")
    
    yield
    
    # Shutdown
    logger.info("üîÑ Shutting down Qwen-Image API Server...")

app = FastAPI(
    title="Qwen-Image API",
    description="Simple API for Qwen-Image generation",
    version="1.0.0",
    lifespan=lifespan
)

class GenerateRequest(BaseModel):
    prompt: str
    negative_prompt: Optional[str] = "blurry, low quality, distorted"  # Default to enable CFG
    width: int = 1024
    height: int = 1024
    num_inference_steps: int = 50
    true_cfg_scale: float = 4.0  # Official DFloat11 default
    seed: Optional[int] = None

class GenerateResponse(BaseModel):
    image: str  # base64 encoded
    seed: int
    success: bool = True

def image_to_base64(image: Image.Image) -> str:
    """Convert PIL image to base64 string"""
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return img_str

# Model loading is now handled in the lifespan function above

@app.get("/")
async def root():
    """Root endpoint"""
    return {"message": "Qwen-Image API is running! üöÄ", "docs": "/docs"}

@app.get("/health")
async def health():
    """Health check endpoint"""
    if pipeline is None:
        return JSONResponse(
            status_code=503,
            content={"status": "unhealthy", "message": "Model not loaded"}
        )
    
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "gpu_name": torch.cuda.get_device_name(),
            "gpu_memory_used": f"{torch.cuda.memory_allocated() / 1024**3:.2f}GB",
            "gpu_memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB"
        }
    
    return {
        "status": "healthy",
        "model": "Qwen-Image",
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        **gpu_info
    }

@app.post("/generate", response_model=GenerateResponse)
async def generate_image(request: GenerateRequest):
    """Generate image from text prompt"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    logger.info(f"üé® Generating: {request.prompt[:100]}...")
    
    try:
        # Set up generator for reproducible results 
        # For DFloat11 with enable_model_cpu_offload, always use cuda
        generator = None
        if request.seed is not None:
            # DFloat11 with enable_model_cpu_offload: generator must be cuda
            device = "cuda"
            generator = torch.Generator(device=device)
            generator.manual_seed(request.seed)
            logger.info(f"üé≤ Generator device: {device}")
        
        # Generate image
        with torch.inference_mode():
            result = pipeline(
                prompt=request.prompt,
                negative_prompt=request.negative_prompt,
                width=request.width,
                height=request.height,
                num_inference_steps=request.num_inference_steps,
                true_cfg_scale=request.true_cfg_scale,  # Correct parameter for Qwen-Image
                generator=generator
            )
        
        # Get the generated image
        image = result.images[0]
        
        # Convert to base64
        image_b64 = image_to_base64(image)
        
        # Get the seed used
        used_seed = request.seed if request.seed is not None else generator.initial_seed() if generator else 0
        
        logger.info("‚úÖ Image generated successfully!")
        
        return GenerateResponse(
            image=image_b64,
            seed=used_seed,
            success=True
        )
        
    except Exception as e:
        logger.error(f"‚ùå Generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    logger.info("üåü Starting Qwen-Image API Server...")
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
EOF

# Make the API script executable
chmod +x /workspace/qwen_api.py

echo "‚úÖ Setup complete!"
echo ""
echo "üöÄ Starting Qwen-Image API server..."
echo "üì° Server will be available on port 8000"
echo "üìñ API docs will be at: /docs"
echo ""

# Start the API server
cd /workspace
python qwen_api.py
